{
  final int nodeId=request.getNodeId();
  final HashMap<Integer,List<Integer>> replicaToPartitionList=ProtoUtils.decodePartitionTuple(request.getReplicaToPartitionList());
  final VoldemortFilter filter=request.hasFilter() ? getFilterFromRequest(request.getFilter(),voldemortConfig,networkClassLoader) : new DefaultVoldemortFilter();
  final String storeName=request.getStore();
  final boolean optimize=request.hasOptimize() ? request.getOptimize() : false;
  final Cluster initialCluster=request.hasInitialCluster() ? new ClusterMapper().readCluster(new StringReader(request.getInitialCluster())) : null;
  int requestId=asyncService.getUniqueRequestId();
  VAdminProto.AsyncOperationStatusResponse.Builder response=VAdminProto.AsyncOperationStatusResponse.newBuilder().setRequestId(requestId).setComplete(false).setDescription("Fetch and update").setStatus("Started");
  final StoreDefinition storeDef=metadataStore.getStoreDef(storeName);
  final boolean isReadOnlyStore=storeDef.getType().compareTo(ReadOnlyStorageConfiguration.TYPE_NAME) == 0;
  final RoutingStrategy oldStrategy=initialCluster != null ? new RoutingStrategyFactory().updateRoutingStrategy(storeDef,initialCluster) : null;
  try {
    asyncService.submitOperation(requestId,new AsyncOperation(requestId,"Fetch and Update"){
      private final AtomicBoolean running=new AtomicBoolean(true);
      @Override public void stop(){
        running.set(false);
      }
      @Override public void operate(){
        AdminClient adminClient=RebalanceUtils.createTempAdminClient(voldemortConfig,metadataStore.getCluster(),4,2);
        try {
          StorageEngine<ByteArray,byte[],byte[]> storageEngine=getStorageEngine(storeRepository,storeName);
          EventThrottler throttler=new EventThrottler(voldemortConfig.getStreamMaxWriteBytesPerSec());
          if (isReadOnlyStore) {
            ReadOnlyStorageEngine readOnlyStorageEngine=((ReadOnlyStorageEngine)storageEngine);
            String destinationDir=readOnlyStorageEngine.getCurrentDirPath();
            logger.info("Fetching files for RO store '" + storeName + "' from node "+ nodeId+ " ( "+ replicaToPartitionList+ " )");
            updateStatus("Fetching files for RO store '" + storeName + "' from node "+ nodeId+ " ( "+ replicaToPartitionList+ " )");
            adminClient.fetchPartitionFiles(nodeId,storeName,replicaToPartitionList,destinationDir,readOnlyStorageEngine.getChunkedFileSet().getChunkIdToNumChunks().keySet());
          }
 else {
            logger.info("Fetching entries for RW store '" + storeName + "' from node "+ nodeId+ " ( "+ replicaToPartitionList+ " )");
            updateStatus("Fetching entries for RW store '" + storeName + "' from node "+ nodeId+ " ( "+ replicaToPartitionList+ " ) ");
            HashMap<Integer,List<Integer>> optimizedReplicaToPartitionList=Maps.newHashMap();
            if (oldStrategy != null && optimize) {
              for (              Entry<Integer,List<Integer>> tuple : replicaToPartitionList.entrySet()) {
                List<Integer> partitionList=Lists.newArrayList();
                for (                int partition : tuple.getValue()) {
                  List<Integer> preferenceList=oldStrategy.getReplicatingPartitionList(partition);
                  if (!RebalanceUtils.containsPreferenceList(initialCluster,preferenceList,metadataStore.getNodeId())) {
                    partitionList.add(partition);
                  }
                }
                if (partitionList.size() > 0) {
                  optimizedReplicaToPartitionList.put(tuple.getKey(),partitionList);
                }
              }
              logger.info("After running RW level optimization - Fetching entries for RW store '" + storeName + "' from node "+ nodeId+ " ( "+ optimizedReplicaToPartitionList+ " )");
              updateStatus("After running RW level optimization - Fetching entries for RW store '" + storeName + "' from node "+ nodeId+ " ( "+ optimizedReplicaToPartitionList+ " )");
            }
 else {
              optimizedReplicaToPartitionList.putAll(replicaToPartitionList);
            }
            if (optimizedReplicaToPartitionList.size() > 0) {
              Iterator<Pair<ByteArray,Versioned<byte[]>>> entriesIterator=adminClient.fetchEntries(nodeId,storeName,optimizedReplicaToPartitionList,filter,false,initialCluster,0);
              long numTuples=0;
              while (running.get() && entriesIterator.hasNext()) {
                Pair<ByteArray,Versioned<byte[]>> entry=entriesIterator.next();
                ByteArray key=entry.getFirst();
                Versioned<byte[]> value=entry.getSecond();
                try {
                  storageEngine.put(key,value,null);
                }
 catch (                ObsoleteVersionException e) {
                  logger.debug("Fetch and update threw Obsolete version exception. Ignoring");
                }
                throttler.maybeThrottle(key.length() + valueSize(value));
                if ((numTuples % 10000) == 0 && numTuples > 0) {
                  logger.info(numTuples + " entries copied from node " + nodeId+ " for store '"+ storeName+ "'");
                  updateStatus(numTuples + " entries copied from node " + nodeId+ " for store '"+ storeName+ "'");
                }
                numTuples++;
              }
              logger.info("Completed fetching " + numTuples + " entries from node "+ nodeId+ " for store '"+ storeName+ "'");
            }
 else {
              logger.info("No entries to fetch from node " + nodeId + " for store '"+ storeName+ "'");
            }
          }
        }
  finally {
          adminClient.stop();
        }
      }
    }
);
  }
 catch (  VoldemortException e) {
    response.setError(ProtoUtils.encodeError(errorCodeMapper,e));
    logger.error("handleFetchAndUpdate failed for request(" + request.toString() + ")",e);
  }
  return response.build();
}
