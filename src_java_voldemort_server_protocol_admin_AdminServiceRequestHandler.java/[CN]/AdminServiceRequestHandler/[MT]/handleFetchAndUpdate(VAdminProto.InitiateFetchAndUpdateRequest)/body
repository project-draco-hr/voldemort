{
  final int nodeId=request.getNodeId();
  final List<Integer> partitionIds=request.getPartitionIdsList();
  final VoldemortFilter filter=request.hasFilter() ? getFilterFromRequest(request.getFilter(),voldemortConfig,networkClassLoader) : new DefaultVoldemortFilter();
  final String storeName=request.getStore();
  final Cluster initialCluster=request.hasInitialCluster() ? new ClusterMapper().readCluster(new StringReader(request.getInitialCluster())) : null;
  int requestId=asyncService.getUniqueRequestId();
  VAdminProto.AsyncOperationStatusResponse.Builder response=VAdminProto.AsyncOperationStatusResponse.newBuilder().setRequestId(requestId).setComplete(false).setDescription("Fetch and update").setStatus("Started");
  final StoreDefinition storeDef=metadataStore.getStoreDef(storeName);
  final boolean isReadOnlyStore=storeDef.getType().compareTo(ReadOnlyStorageConfiguration.TYPE_NAME) == 0;
  try {
    asyncService.submitOperation(requestId,new AsyncOperation(requestId,"Fetch and Update"){
      private final AtomicBoolean running=new AtomicBoolean(true);
      @Override public void stop(){
        running.set(false);
        logger.info("Stopping fetch and update for store " + storeName + " from node "+ nodeId+ "( "+ partitionIds+ " )");
      }
      @Override public void operate(){
        AdminClient adminClient=AdminClient.createTempAdminClient(voldemortConfig,metadataStore.getCluster(),voldemortConfig.getClientMaxConnectionsPerNode());
        try {
          StorageEngine<ByteArray,byte[],byte[]> storageEngine=getStorageEngine(storeRepository,storeName);
          EventThrottler throttler=new EventThrottler(voldemortConfig.getStreamMaxWriteBytesPerSec());
          if (isReadOnlyStore) {
            ReadOnlyStorageEngine readOnlyStorageEngine=((ReadOnlyStorageEngine)storageEngine);
            String destinationDir=readOnlyStorageEngine.getCurrentDirPath();
            logger.info("Fetching files for RO store '" + storeName + "' from node "+ nodeId+ " ( "+ partitionIds+ " )");
            updateStatus("Fetching files for RO store '" + storeName + "' from node "+ nodeId+ " ( "+ partitionIds+ " )");
            adminClient.readonlyOps.fetchPartitionFiles(nodeId,storeName,partitionIds,destinationDir,readOnlyStorageEngine.getChunkedFileSet().getChunkIdToNumChunks().keySet(),running);
          }
 else {
            logger.info("Fetching entries for RW store '" + storeName + "' from node "+ nodeId+ " ( "+ partitionIds+ " )");
            updateStatus("Fetching entries for RW store '" + storeName + "' from node "+ nodeId+ " ( "+ partitionIds+ " ) ");
            if (partitionIds.size() > 0) {
              Iterator<Pair<ByteArray,Versioned<byte[]>>> entriesIterator=adminClient.bulkFetchOps.fetchEntries(nodeId,storeName,partitionIds,filter,false,initialCluster,0);
              long numTuples=0;
              long startTime=System.currentTimeMillis();
              while (running.get() && entriesIterator.hasNext()) {
                Pair<ByteArray,Versioned<byte[]>> entry=entriesIterator.next();
                ByteArray key=entry.getFirst();
                Versioned<byte[]> value=entry.getSecond();
                try {
                  storageEngine.put(key,value,null);
                }
 catch (                ObsoleteVersionException e) {
                  logger.debug("Fetch and update threw Obsolete version exception. Ignoring");
                }
                long totalTime=(System.currentTimeMillis() - startTime) / 1000;
                throttler.maybeThrottle(key.length() + valueSize(value));
                if ((numTuples % 100000) == 0 && numTuples > 0) {
                  logger.info(numTuples + " entries copied from node " + nodeId+ " for store '"+ storeName+ "'c");
                  updateStatus(numTuples + " entries copied from node " + nodeId+ " for store '"+ storeName+ "' in "+ totalTime+ " seconds");
                }
                numTuples++;
              }
              long totalTime=(System.currentTimeMillis() - startTime) / 1000;
              if (running.get()) {
                logger.info("Completed fetching " + numTuples + " entries from node "+ nodeId+ " for store '"+ storeName+ "' in "+ totalTime+ " seconds");
              }
 else {
                logger.info("Fetch and update stopped after fetching " + numTuples + " entries for node "+ nodeId+ " for store '"+ storeName+ "' in "+ totalTime+ " seconds");
              }
            }
 else {
              logger.info("No entries to fetch from node " + nodeId + " for store '"+ storeName+ "'");
            }
          }
        }
  finally {
          adminClient.close();
        }
      }
    }
);
  }
 catch (  VoldemortException e) {
    response.setError(ProtoUtils.encodeError(errorCodeMapper,e));
    logger.error("handleFetchAndUpdate failed for request(" + request.toString() + ")",e);
  }
  return response.build();
}
