{
  JobConf conf=new JobConf(config);
  conf.setInt("io.file.buffer.size",DEFAULT_BUFFER_SIZE);
  conf.set("cluster.xml",new ClusterMapper().writeCluster(cluster));
  conf.set("stores.xml",new StoreDefinitionsMapper().writeStoreList(Collections.singletonList(storeDef)));
  conf.setInt("hadoop.node.id",this.hadoopNodeId);
  conf.setLong("hadoop.push.version",this.hadoopPushVersion);
  conf.setLong("job.start.time.ms",System.currentTimeMillis());
  conf.setPartitionerClass(HadoopRWStoreBuilderPartitioner.class);
  conf.setInputFormat(inputFormatClass);
  conf.setMapperClass(mapperClass);
  conf.setMapOutputKeyClass(BytesWritable.class);
  conf.setMapOutputValueClass(BytesWritable.class);
  conf.setReducerClass(HadoopRWStoreBuilderReducer.class);
  conf.setOutputFormat(SequenceFileOutputFormat.class);
  conf.setOutputKeyClass(BytesWritable.class);
  conf.setOutputValueClass(BytesWritable.class);
  conf.setJarByClass(getClass());
  FileInputFormat.setInputPaths(conf,inputPath);
  FileOutputFormat.setOutputPath(conf,tempPath);
  try {
    FileSystem tempFs=tempPath.getFileSystem(conf);
    tempFs.delete(tempPath,true);
    conf.setInt("num.chunks",reducersPerNode);
    int numReducers=cluster.getNumberOfNodes() * reducersPerNode;
    logger.info("Replication factor = " + storeDef.getReplicationFactor() + ", numNodes = "+ cluster.getNumberOfNodes()+ ", reducers per node = "+ reducersPerNode+ ", numReducers = "+ numReducers);
    conf.setNumReduceTasks(numReducers);
    logger.info("Building RW store...");
    JobClient.runJob(conf);
  }
 catch (  Exception e) {
    throw new VoldemortException(e);
  }
}
