{
  addThrottledJob();
  ObjectName jmxName=null;
  try {
    RestHadoopAuth.loginSecuredHdfs();
    sourceFileUrl=sourceFileUrl.replace("webhdfs","http");
    URL sourceUrl=new URL(sourceFileUrl);
    RestFileSystem rfs=new RestFileSystem(sourceUrl.getProtocol() + "://" + sourceUrl.getHost()+ ":"+ sourceUrl.getPort());
    String fullyQualifiedFileName=sourceUrl.getFile();
    CopyStats stats=new CopyStats(fullyQualifiedFileName,sizeOfPath(rfs,fullyQualifiedFileName));
    jmxName=JmxUtils.registerMbean("hdfs-copy-" + copyCount.getAndIncrement(),stats);
    File destination=new File(destinationFile);
    if (destination.exists()) {
      throw new VoldemortException("Version directory " + destination.getAbsolutePath() + " already exists");
    }
    logger.info("Starting fetch for : " + sourceFileUrl);
    boolean result=fetch(rfs,fullyQualifiedFileName,destination,stats);
    logger.info("Completed fetch : " + sourceFileUrl);
    if (result) {
      return destination;
    }
 else {
      return null;
    }
  }
 catch (  RestFSException rfse) {
    rfse.printStackTrace();
    logger.error("Encountered exception while accessing hadoop via RestHdfsClient : " + rfse);
    throw new VoldemortException("Error while accessing hadoop via RestHdfsClient : " + rfse);
  }
catch (  Throwable te) {
    te.printStackTrace();
    logger.error("Error thrown while trying to get Hadoop filesystem");
    throw new VoldemortException("Error thrown while trying to get Hadoop filesystem : " + te);
  }
 finally {
    removeThrottledJob();
    if (jmxName != null)     JmxUtils.unregisterMbean(jmxName);
  }
}
