{
  init();
  generateUnCompressedFiles(saveKeys);
  conf.setBoolean(VoldemortBuildAndPushJob.SAVE_KEYS,saveKeys);
  conf.setBoolean(VoldemortBuildAndPushJob.REDUCER_OUTPUT_COMPRESS,true);
  conf.setStrings(VoldemortBuildAndPushJob.REDUCER_OUTPUT_COMPRESS_CODEC,KeyValueWriter.COMPRESSION_CODEC);
  hadoopStoreWriterPerBucket=new HadoopStoreWriter();
  hadoopStoreWriterPerBucket.conf(conf);
  for (int i=0; i < numKeys; i++) {
    String key="test_key_" + i;
    String value="test_value_" + i;
    mapper.map(key.getBytes(),value.getBytes(),testCollectorWrapper);
    hadoopStoreWriterPerBucket.write(testCollector.key,testCollector.values.iterator(),null);
  }
  hadoopStoreWriterPerBucket.close();
  for (int i=0; i < numChunks; i++) {
    generateExpectedDataAndIndexFileNames(true,i);
    String decompressedDataFile=dataFile.getAbsolutePath() + "_decompressed";
    ReadOnlyTestUtils.unGunzipFile(dataFile.getAbsolutePath(),decompressedDataFile);
    String decompressedIndexFile=indexFile.getAbsolutePath() + "_decompressed";
    ReadOnlyTestUtils.unGunzipFile(indexFile.getAbsolutePath(),decompressedIndexFile);
    generateExpectedDataAndIndexFileNames(false,i);
    Assert.assertTrue(ReadOnlyTestUtils.areTwoBinaryFilesEqual(dataFile,new File(decompressedDataFile)));
    Assert.assertTrue(ReadOnlyTestUtils.areTwoBinaryFilesEqual(indexFile,new File(decompressedIndexFile)));
  }
  cleanUp();
}
