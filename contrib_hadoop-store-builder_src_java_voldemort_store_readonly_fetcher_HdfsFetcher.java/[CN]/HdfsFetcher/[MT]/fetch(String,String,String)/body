{
  if (this.globalThrottleLimit != null) {
    if (this.globalThrottleLimit.getSpeculativeRate() < this.minBytesPerSecond)     throw new VoldemortException("Too many push jobs.");
    this.globalThrottleLimit.incrementNumJobs();
  }
  ObjectName jmxName=null;
  try {
    final FileSystem fs=getHadoopFileSystem(sourceFileUrl,hadoopConfigPath);
    final Path path=new Path(sourceFileUrl);
    CopyStats stats=new CopyStats(sourceFileUrl,sizeOfPath(fs,path));
    jmxName=JmxUtils.registerMbean("hdfs-copy-" + copyCount.getAndIncrement(),stats);
    File destination=new File(destinationFile);
    if (destination.exists()) {
      throw new VoldemortException("Version directory " + destination.getAbsolutePath() + " already exists");
    }
    logger.info("Starting fetch for : " + sourceFileUrl);
    boolean result=fetch(fs,path,destination,stats);
    logger.info("Completed fetch : " + sourceFileUrl);
    fs.close();
    if (result) {
      return destination;
    }
 else {
      return null;
    }
  }
 catch (  Throwable te) {
    te.printStackTrace();
    logger.error("Error thrown while trying to get data from Hadoop filesystem",te);
    throw new VoldemortException("Error thrown while trying to get data from Hadoop filesystem : " + te);
  }
 finally {
    if (this.globalThrottleLimit != null) {
      this.globalThrottleLimit.decrementNumJobs();
    }
    if (jmxName != null)     JmxUtils.unregisterMbean(jmxName);
  }
}
