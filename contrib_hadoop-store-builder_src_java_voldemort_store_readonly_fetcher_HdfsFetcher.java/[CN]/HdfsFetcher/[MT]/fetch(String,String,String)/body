{
  if (this.globalThrottleLimit != null) {
    if (this.globalThrottleLimit.getSpeculativeRate() < this.minBytesPerSecond)     throw new VoldemortException("Too many push jobs.");
    this.globalThrottleLimit.incrementNumJobs();
  }
  ObjectName jmxName=null;
  try {
    final Configuration config=new Configuration();
    FileSystem fs=null;
    config.setInt("io.socket.receive.buffer",bufferSize);
    config.set("hadoop.rpc.socket.factory.class.ClientProtocol",ConfigurableSocketFactory.class.getName());
    config.set("hadoop.security.group.mapping","org.apache.hadoop.security.ShellBasedUnixGroupsMapping");
    final Path path=new Path(sourceFileUrl);
    if (hadoopConfigPath.length() > 0) {
      config.addResource(new Path(hadoopConfigPath + "/core-site.xml"));
      config.addResource(new Path(hadoopConfigPath + "/hdfs-site.xml"));
      String security=config.get(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION);
      if (security == null || !security.equals("kerberos")) {
        logger.info("Security isn't turned on in the conf: " + CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION + " = "+ config.get(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION));
        logger.info("Fix that.  Exiting.");
        return null;
      }
 else {
        logger.info("Security is turned on in the conf. Trying to authenticate ...");
      }
    }
    try {
      if (HdfsFetcher.keytabPath.length() > 0) {
        UserGroupInformation.setConfiguration(config);
        UserGroupInformation.loginUserFromKeytab(HdfsFetcher.kerberosPrincipal,HdfsFetcher.keytabPath);
        try {
          logger.info("I've logged in and am now Doasing as " + UserGroupInformation.getCurrentUser().getUserName());
          fs=UserGroupInformation.getCurrentUser().doAs(new PrivilegedExceptionAction<FileSystem>(){
            public FileSystem run() throws Exception {
              FileSystem fs=path.getFileSystem(config);
              return fs;
            }
          }
);
        }
 catch (        InterruptedException e) {
          logger.error(e.getMessage());
        }
catch (        Exception e) {
          logger.error("Got an exception while getting the filesystem object: ");
          logger.error("Exception class : " + e.getClass());
          e.printStackTrace();
          for (          StackTraceElement et : e.getStackTrace()) {
            logger.error(et.toString());
          }
        }
      }
 else {
        fs=path.getFileSystem(config);
      }
    }
 catch (    IOException e) {
      e.printStackTrace();
      logger.error("Error in authenticating or getting the Filesystem object !!! Exiting !!!");
      System.exit(-1);
    }
    CopyStats stats=new CopyStats(sourceFileUrl,sizeOfPath(fs,path));
    jmxName=JmxUtils.registerMbean("hdfs-copy-" + copyCount.getAndIncrement(),stats);
    File destination=new File(destinationFile);
    if (destination.exists()) {
      throw new VoldemortException("Version directory " + destination.getAbsolutePath() + " already exists");
    }
    logger.info("Starting fetch for : " + sourceFileUrl);
    boolean result=fetch(fs,path,destination,stats);
    logger.info("Completed fetch : " + sourceFileUrl);
    fs.close();
    if (result) {
      return destination;
    }
 else {
      return null;
    }
  }
 catch (  Exception e) {
    logger.error("Error while getting Hadoop filesystem : " + e);
    return null;
  }
 finally {
    if (this.globalThrottleLimit != null) {
      this.globalThrottleLimit.decrementNumJobs();
    }
    if (jmxName != null)     JmxUtils.unregisterMbean(jmxName);
  }
}
