{
  ObjectName jmxName=null;
  HdfsCopyStats stats=null;
  FileSystem fs=null;
  sourceFileUrl=VoldemortUtils.modifyURL(sourceFileUrl,voldemortConfig.getModifiedProtocol(),voldemortConfig.getModifiedPort());
  try {
    fs=HadoopUtils.getHadoopFileSystem(voldemortConfig,sourceFileUrl);
    final Path path=new Path(sourceFileUrl);
    File destination=new File(destinationFile);
    if (destination.exists()) {
      throw new VoldemortException("Version directory " + destination.getAbsolutePath() + " already exists");
    }
    boolean isFile=fs.isFile(path);
    stats=new HdfsCopyStats(sourceFileUrl,destination,enableStatsFile,maxVersionsStatsFile,isFile,new HdfsPathInfo(fs,path));
    jmxName=JmxUtils.registerMbean("hdfs-copy-" + copyCount.getAndIncrement(),stats);
    logger.info("Starting fetch for : " + sourceFileUrl);
    FetchStrategy fetchStrategy=new BasicFetchStrategy(this,fs,stats,status,bufferSize);
    if (!fs.isFile(path)) {
      Utils.mkdirs(destination);
      HdfsDirectory directory=new HdfsDirectory(fs,path);
      HdfsFile metadataFile=directory.getMetadataFile();
      Long estimatedDiskSize=-1L;
      if (metadataFile != null) {
        File copyLocation=new File(destination,metadataFile.getPath().getName());
        fetchStrategy.fetch(metadataFile,copyLocation,null);
        directory.initializeMetadata(copyLocation);
        String diskSizeInBytes=(String)directory.getMetadata().get(ReadOnlyStorageMetadata.DISK_SIZE_IN_BYTES);
        estimatedDiskSize=(diskSizeInBytes != null && diskSizeInBytes != "") ? (Long.parseLong(diskSizeInBytes)) : -1L;
      }
      if (diskQuotaSizeInKB != null && diskQuotaSizeInKB != VoldemortConfig.DEFAULT_STORAGE_SPACE_QUOTA_IN_KB) {
        checkIfQuotaExceeded(diskQuotaSizeInKB,storeName,destination,estimatedDiskSize);
      }
 else {
        if (logger.isDebugEnabled()) {
          logger.debug("store: " + storeName + " is a Non Quota type store.");
        }
      }
      Map<HdfsFile,byte[]> fileCheckSumMap=fetchStrategy.fetch(directory,destination);
      if (directory.validateCheckSum(fileCheckSumMap)) {
        logger.info("Completed fetch : " + sourceFileUrl);
        return destination;
      }
 else {
        logger.error("Checksum did not match!");
        return null;
      }
    }
 else     if (allowFetchOfFiles) {
      Utils.mkdirs(destination);
      HdfsFile file=new HdfsFile(fs.getFileStatus(path));
      String fileName=file.getDiskFileName();
      File copyLocation=new File(destination,fileName);
      fetchStrategy.fetch(file,copyLocation,CheckSumType.NONE);
      logger.info("Completed fetch : " + sourceFileUrl);
      return destination;
    }
 else {
      logger.error("Source " + path.toString() + " should be a directory");
      return null;
    }
  }
 catch (  Exception e) {
    if (stats != null) {
      stats.reportError("File fetcher failed for destination " + destinationFile,e);
    }
    if (e instanceof VoldemortException) {
      throw e;
    }
 else {
      throw new VoldemortException("Error thrown while trying to get data from Hadoop filesystem: " + e.getMessage(),e);
    }
  }
 finally {
    if (jmxName != null)     JmxUtils.unregisterMbean(jmxName);
    if (stats != null) {
      stats.complete();
    }
    if (fs != null) {
      try {
        fs.close();
      }
 catch (      IOException e) {
        String errorMessage="Got IOException while trying to close the filesystem instance (harmless).";
        if (stats != null) {
          stats.reportError(errorMessage,e);
        }
        logger.info(errorMessage,e);
      }
    }
  }
}
