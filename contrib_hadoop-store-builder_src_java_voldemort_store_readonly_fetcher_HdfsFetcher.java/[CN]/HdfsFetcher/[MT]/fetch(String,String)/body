{
  if (this.globalThrottleLimit != null) {
    if (this.globalThrottleLimit.getSpeculativeRate() < this.minBytesPerSecond)     throw new VoldemortException("Too many push jobs.");
    this.globalThrottleLimit.incrementNumJobs();
  }
  ObjectName jmxName=null;
  try {
    final Path path=new Path(sourceFileUrl);
    final Configuration config=new Configuration();
    config.setInt("io.socket.receive.buffer",bufferSize);
    config.set("hadoop.rpc.socket.factory.class.ClientProtocol",ConfigurableSocketFactory.class.getName());
    FileSystem fs=null;
    HdfsFetcher.addPath(this.voldemortConfig.getHadoopConfigPath());
synchronized (this) {
      if (this.keytabLocation.length() > 0) {
        logger.info("keytab path = " + keytabLocation + " and proxy user = "+ proxyUser);
        UserGroupInformation.loginUserFromKeytab(proxyUser,keytabLocation);
        logger.info("I've logged in and am now Doasing as " + UserGroupInformation.getCurrentUser().getUserName());
        try {
          fs=UserGroupInformation.getCurrentUser().doAs(new PrivilegedExceptionAction<FileSystem>(){
            public FileSystem run() throws Exception {
              FileSystem fs=path.getFileSystem(config);
              return fs;
            }
          }
);
        }
 catch (        InterruptedException e) {
          logger.error(e.getMessage());
          return null;
        }
catch (        Exception e) {
          logger.error("Got an exception while getting the filesystem object: ");
          logger.error("Exception class : " + e.getClass());
          e.printStackTrace();
          for (          StackTraceElement et : e.getStackTrace()) {
            logger.error(et.toString());
          }
        }
      }
 else {
        fs=path.getFileSystem(config);
      }
    }
    CopyStats stats=new CopyStats(sourceFileUrl,sizeOfPath(fs,path));
    jmxName=JmxUtils.registerMbean("hdfs-copy-" + copyCount.getAndIncrement(),stats);
    File destination=new File(destinationFile);
    if (destination.exists()) {
      throw new VoldemortException("Version directory " + destination.getAbsolutePath() + " already exists");
    }
    logger.info("Starting fetch for : " + sourceFileUrl);
    boolean result=fetch(fs,path,destination,stats);
    logger.info("Completed fetch : " + sourceFileUrl);
    fs.close();
    if (result) {
      return destination;
    }
 else {
      return null;
    }
  }
 catch (  Exception e) {
    logger.error("Error while getting Hadoop filesystem : " + e);
    return null;
  }
 finally {
    if (this.globalThrottleLimit != null) {
      this.globalThrottleLimit.decrementNumJobs();
    }
    if (jmxName != null)     JmxUtils.unregisterMbean(jmxName);
  }
}
