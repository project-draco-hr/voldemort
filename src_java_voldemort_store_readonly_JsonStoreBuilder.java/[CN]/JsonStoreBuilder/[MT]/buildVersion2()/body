{
  logger.info("Building store " + storeDefinition.getName() + " for "+ cluster.getNumberOfPartitions()+ " partitions, "+ storeDefinition.getReplicationFactor()+ " replica types, "+ numChunks+ " chunks per partitions per replica type and type "+ ReadOnlyStorageFormat.READONLY_V2);
  int numNodes=cluster.getNumberOfNodes();
  DataOutputStream[][] indexes=new DataOutputStream[numNodes][];
  DataOutputStream[][] datas=new DataOutputStream[numNodes][];
  int[][] positions=new int[numNodes][];
  HashMap<Pair<Integer,Integer>,Pair<byte[],byte[]>> previousElements=Maps.newHashMap();
  int[] partitionIdToChunkOffset=new int[cluster.getNumberOfPartitions()];
  int[] partitionIdToNodeId=new int[cluster.getNumberOfPartitions()];
  for (  Node node : cluster.getNodes()) {
    int nodeId=node.getId();
    indexes[nodeId]=new DataOutputStream[node.getNumberOfPartitions() * storeDefinition.getReplicationFactor() * numChunks];
    datas[nodeId]=new DataOutputStream[node.getNumberOfPartitions() * storeDefinition.getReplicationFactor() * numChunks];
    positions[nodeId]=new int[node.getNumberOfPartitions() * storeDefinition.getReplicationFactor() * numChunks];
    File nodeDir=new File(outputDir,"node-" + Integer.toString(nodeId));
    nodeDir.mkdirs();
    BufferedWriter writer=new BufferedWriter(new FileWriter(new File(nodeDir,".metadata")));
    ReadOnlyStorageMetadata metadata=new ReadOnlyStorageMetadata();
    metadata.add(ReadOnlyStorageMetadata.FORMAT,ReadOnlyStorageFormat.READONLY_V2.getCode());
    writer.write(metadata.toJsonString());
    writer.close();
    int globalChunk=0;
    for (    Integer partition : node.getPartitionIds()) {
      partitionIdToChunkOffset[partition]=globalChunk;
      partitionIdToNodeId[partition]=node.getId();
      for (int repType=0; repType < storeDefinition.getReplicationFactor(); repType++) {
        for (int chunk=0; chunk < numChunks; chunk++) {
          File indexFile=new File(nodeDir,Integer.toString(partition) + "_" + Integer.toString(repType)+ "_"+ Integer.toString(chunk)+ ".index");
          File dataFile=new File(nodeDir,Integer.toString(partition) + "_" + Integer.toString(repType)+ "_"+ Integer.toString(chunk)+ ".data");
          positions[nodeId][globalChunk]=0;
          indexes[nodeId][globalChunk]=new DataOutputStream(new BufferedOutputStream(new FileOutputStream(indexFile),ioBufferSize));
          datas[nodeId][globalChunk]=new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dataFile),ioBufferSize));
          globalChunk++;
        }
      }
    }
  }
  logger.info("Reading items...");
  ExternalSorter<KeyValuePair> sorter=new ExternalSorter<KeyValuePair>(new KeyValuePairSerializer(),new KeyMd5Comparator(),internalSortSize,tempDir.getAbsolutePath(),ioBufferSize,numThreads,gzipIntermediate);
  JsonObjectIterator iter=new JsonObjectIterator(reader,storeDefinition);
  int count=0;
  for (  KeyValuePair currentElement : sorter.sorted(iter)) {
    List<Integer> partitionIds=this.routingStrategy.getPartitionList(currentElement.getKey());
    int localChunkId=ReadOnlyUtils.chunk(currentElement.getKeyMd5(),numChunks);
    int replicaType=0;
    for (    Integer partitionId : partitionIds) {
      int nodeId=partitionIdToNodeId[partitionId];
      int globalChunkId=partitionIdToChunkOffset[partitionId] + (replicaType * numChunks) + localChunkId;
      Pair<Integer,Integer> key=Pair.create(nodeId,globalChunkId);
      if (!previousElements.containsKey(key)) {
        previousElements.put(key,Pair.create(ByteUtils.copy(currentElement.getKeyMd5(),0,2 * ByteUtils.SIZE_OF_INT),generateFirstElement(currentElement)));
      }
 else {
        Pair<byte[],byte[]> previousElement=previousElements.get(key);
        if (ByteUtils.compare(previousElement.getFirst(),currentElement.getKeyMd5(),0,2 * ByteUtils.SIZE_OF_INT) == 0) {
          short numKeys=ByteUtils.readShort(previousElement.getSecond(),0);
          ByteArrayOutputStream stream=new ByteArrayOutputStream();
          DataOutputStream valueStream=new DataOutputStream(stream);
          valueStream.writeShort(numKeys + 1);
          valueStream.write(ByteUtils.copy(previousElement.getSecond(),ByteUtils.SIZE_OF_SHORT,previousElement.getSecond().length));
          valueStream.writeInt(currentElement.getKey().length);
          valueStream.writeInt(currentElement.getValue().length);
          valueStream.write(currentElement.getKey());
          valueStream.write(currentElement.getValue());
          valueStream.flush();
          previousElements.put(key,Pair.create(previousElement.getFirst(),stream.toByteArray()));
        }
 else {
          indexes[nodeId][globalChunkId].write(previousElement.getFirst());
          indexes[nodeId][globalChunkId].writeInt(positions[nodeId][globalChunkId]);
          datas[nodeId][globalChunkId].write(previousElement.getSecond());
          positions[nodeId][globalChunkId]+=previousElement.getSecond().length;
          previousElements.put(key,Pair.create(ByteUtils.copy(currentElement.getKeyMd5(),0,2 * ByteUtils.SIZE_OF_INT),generateFirstElement(currentElement)));
        }
      }
      replicaType++;
    }
    count++;
  }
  logger.info(count + " items read.");
  for (  Entry<Pair<Integer,Integer>,Pair<byte[],byte[]>> entry : previousElements.entrySet()) {
    int nodeId=entry.getKey().getFirst();
    int globalChunkId=entry.getKey().getSecond();
    byte[] keyMd5=entry.getValue().getFirst();
    byte[] value=entry.getValue().getSecond();
    indexes[nodeId][globalChunkId].write(keyMd5);
    indexes[nodeId][globalChunkId].writeInt(positions[nodeId][globalChunkId]);
    datas[nodeId][globalChunkId].write(value);
  }
  logger.info("Closing all store files.");
  for (  Node node : cluster.getNodes()) {
    for (int chunk=0; chunk < numChunks * storeDefinition.getReplicationFactor() * node.getNumberOfPartitions(); chunk++) {
      indexes[node.getId()][chunk].close();
      datas[node.getId()][chunk].close();
    }
  }
}
