{
  for (int chunkId=0; chunkId < getNumChunks(); chunkId++) {
    this.indexFileStream[chunkId].close();
    this.valueFileStream[chunkId].close();
  }
  if (this.nodeId == -1 || this.partitionId == -1) {
    return;
  }
  if (getSaveKeys() && this.replicaType == -1) {
    throw new RuntimeException("Could not read the replica type correctly for node " + nodeId + " ( partition - "+ this.partitionId+ " )");
  }
  String fileNamePrefix=null;
  if (getSaveKeys()) {
    fileNamePrefix=new String(Integer.toString(this.partitionId) + "_" + Integer.toString(this.replicaType)+ "_");
  }
 else {
    fileNamePrefix=new String(Integer.toString(this.partitionId) + "_");
  }
  Path nodeDir=new Path(this.outputDir,"node-" + this.nodeId);
  FileSystem outputFs=nodeDir.getFileSystem(this.conf);
  outputFs.mkdirs(nodeDir);
  outputFs.setPermission(nodeDir,new FsPermission(HadoopStoreBuilder.HADOOP_FILE_PERMISSION));
  logger.info("Setting permission to 755 for " + nodeDir);
  for (int chunkId=0; chunkId < getNumChunks(); chunkId++) {
    String chunkFileName=fileNamePrefix + Integer.toString(chunkId);
    CheckSumMetadata indexCheckSum=new CheckSumMetadata();
    CheckSumMetadata valueCheckSum=new CheckSumMetadata();
    if (this.checkSumType != CheckSumType.NONE) {
      if (this.checkSumDigestIndex[chunkId] != null && this.checkSumDigestValue[chunkId] != null) {
        indexCheckSum.add(ReadOnlyStorageMetadata.CHECKSUM,new String(Hex.encodeHex(this.checkSumDigestIndex[chunkId].getCheckSum())));
        valueCheckSum.add(ReadOnlyStorageMetadata.CHECKSUM,new String(Hex.encodeHex(this.checkSumDigestValue[chunkId].getCheckSum())));
      }
 else {
        throw new RuntimeException("Failed to open checksum digest for node " + nodeId + " ( partition - "+ this.partitionId+ ", chunk - "+ chunkId+ " )");
      }
    }
    Path checkSumIndexFile=new Path(nodeDir,chunkFileName + INDEX_FILE_EXTENSION + CHECKSUM_FILE_EXTENSION);
    Path checkSumValueFile=new Path(nodeDir,chunkFileName + DATA_FILE_EXTENSION + CHECKSUM_FILE_EXTENSION);
    if (outputFs.exists(checkSumIndexFile)) {
      outputFs.delete(checkSumIndexFile);
    }
    FSDataOutputStream output=outputFs.create(checkSumIndexFile);
    outputFs.setPermission(checkSumIndexFile,new FsPermission(HadoopStoreBuilder.HADOOP_FILE_PERMISSION));
    indexCheckSum.add(CheckSumMetadata.INDEX_FILE_SIZE_IN_BYTES,Long.toString(this.indexFileSizeInBytes[chunkId]));
    output.write(indexCheckSum.toJsonString().getBytes());
    output.close();
    if (outputFs.exists(checkSumValueFile)) {
      outputFs.delete(checkSumValueFile);
    }
    output=outputFs.create(checkSumValueFile);
    outputFs.setPermission(checkSumValueFile,new FsPermission(HadoopStoreBuilder.HADOOP_FILE_PERMISSION));
    valueCheckSum.add(CheckSumMetadata.DATA_FILE_SIZE_IN_BYTES,Long.toString(this.valueFileSizeInBytes[chunkId]));
    output.write(valueCheckSum.toJsonString().getBytes());
    output.close();
    Path indexFile=new Path(nodeDir,chunkFileName + INDEX_FILE_EXTENSION + fileExtension);
    Path valueFile=new Path(nodeDir,chunkFileName + DATA_FILE_EXTENSION + fileExtension);
    logger.info("Moving " + this.taskIndexFileName[chunkId] + " to "+ indexFile);
    if (outputFs.exists(indexFile)) {
      outputFs.delete(indexFile);
    }
    fs.rename(taskIndexFileName[chunkId],indexFile);
    logger.info("Moving " + this.taskValueFileName[chunkId] + " to "+ valueFile);
    if (outputFs.exists(valueFile)) {
      outputFs.delete(valueFile);
    }
    fs.rename(this.taskValueFileName[chunkId],valueFile);
  }
}
