{
  JobConf configuration=this.createJobConf(VoldemortStoreBuilderMapper.class);
  Class mapperClass;
  Class<? extends InputFormat> inputFormatClass;
  if (conf.isAvro()) {
    configuration.set("avro.rec.schema",conf.getRecSchema());
    configuration.set("avro.key.schema",conf.getKeySchema());
    configuration.set("avro.val.schema",conf.getValSchema());
    configuration.set("avro.key.field",conf.getKeyField());
    configuration.set("avro.value.field",conf.getValueField());
    mapperClass=AvroStoreBuilderMapper.class;
    inputFormatClass=AvroInputFormat.class;
  }
 else {
    mapperClass=VoldemortStoreBuilderMapper.class;
    inputFormatClass=JsonSequenceFileInputFormat.class;
  }
  String storeName=conf.getStoreName();
  StoreDefinition storeDef=null;
  for (  StoreDefinition def : conf.getStoreDefs())   if (storeName.equals(def.getName()))   storeDef=def;
  if (storeDef == null)   throw new IllegalArgumentException("Store '" + storeName + "' not found.");
  Path outputDir=conf.getOutputDir();
  FileSystem fs=outputDir.getFileSystem(configuration);
  if (fs.exists(outputDir)) {
    info("Deleting previous output in " + outputDir + " for building store "+ storeName);
    fs.delete(outputDir,true);
  }
  HadoopStoreBuilder builder=new HadoopStoreBuilder(configuration,mapperClass,inputFormatClass,conf.getCluster(),storeDef,conf.getTempDir(),outputDir,conf.getInputPath(),conf.getCheckSumType(),conf.getSaveKeys(),conf.getReducerPerBucket(),conf.getChunkSize(),conf.getNumChunks(),conf.isAvro(),conf.getMinNumberOfRecords());
  builder.build();
}
