{
  JobConf configuration=this.createJobConf(VoldemortStoreBuilderMapper.class);
  if (conf.isAvro()) {
    String recSchema=conf.getRecSchema();
    String keySchema=conf.getKeySchema();
    String valSchema=conf.getValSchema();
    String keyField=conf.getKeyField();
    String valueField=conf.getValueField();
    configuration.set("avro.rec.schema",recSchema);
    configuration.set("avro.key.schema",keySchema);
    configuration.set("avro.val.schema",valSchema);
    configuration.set("avro.key.field",keyField);
    configuration.set("avro.value.field",valueField);
  }
  int chunkSize=conf.getChunkSize();
  Path tempDir=conf.getTempDir();
  Path outputDir=conf.getOutputDir();
  Path inputPath=conf.getInputPath();
  Cluster cluster=conf.getCluster();
  List<StoreDefinition> storeDefs=conf.getStoreDefs();
  String storeName=conf.getStoreName();
  CheckSumType checkSumType=conf.getCheckSumType();
  boolean saveKeys=conf.getSaveKeys();
  boolean reducerPerBucket=conf.getReducerPerBucket();
  StoreDefinition storeDef=null;
  for (  StoreDefinition def : storeDefs)   if (storeName.equals(def.getName()))   storeDef=def;
  if (storeDef == null)   throw new IllegalArgumentException("Store '" + storeName + "' not found.");
  FileSystem fs=outputDir.getFileSystem(configuration);
  if (fs.exists(outputDir)) {
    info("Deleting previous output in " + outputDir + " for building store "+ storeName);
    fs.delete(outputDir,true);
  }
  HadoopStoreBuilder builder;
  Class mapperClass;
  Class<? extends InputFormat> inputFormatClass;
  if (conf.isAvro()) {
    mapperClass=AvroStoreBuilderMapper.class;
    inputFormatClass=AvroInputFormat.class;
  }
 else {
    mapperClass=VoldemortStoreBuilderMapper.class;
    inputFormatClass=JsonSequenceFileInputFormat.class;
  }
  builder=new HadoopStoreBuilder(configuration,mapperClass,inputFormatClass,cluster,storeDef,tempDir,outputDir,inputPath,checkSumType,saveKeys,reducerPerBucket,chunkSize,conf.getNumChunks(),conf.isAvro(),conf.getMinNumberOfRecords());
  builder.build();
}
