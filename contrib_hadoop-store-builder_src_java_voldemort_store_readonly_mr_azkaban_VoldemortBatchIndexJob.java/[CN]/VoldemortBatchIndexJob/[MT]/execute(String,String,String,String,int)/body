{
  JobConf conf=createJobConf(VoldemortBatchIndexMapper.class,VoldemortBatchIndexReducer.class);
  try {
    _cluster=HadoopUtils.readCluster(voldemortClusterLocalFile,conf);
  }
 catch (  Exception e) {
    logger.error("Failed to read Voldemort cluster details",e);
    throw new RuntimeException("",e);
  }
  conf.setPartitionerClass(VoldemortBatchIndexPartitoner.class);
  conf.setNumReduceTasks(_cluster.getNumberOfNodes());
  FileInputFormat.setInputPaths(conf,inputPath);
  FileOutputFormat.setOutputPath(conf,new Path(outputPath));
  if (getProps().getBoolean("force.output.overwrite",false)) {
    FileSystem fs=FileOutputFormat.getOutputPath(conf).getFileSystem(conf);
    fs.delete(FileOutputFormat.getOutputPath(conf),true);
  }
  conf.setInputFormat(SequenceFileInputFormat.class);
  conf.setOutputFormat(SequenceFileOutputFormat.class);
  conf.setMapOutputKeyClass(BytesWritable.class);
  conf.setMapOutputValueClass(BytesWritable.class);
  conf.setOutputKeyClass(BytesWritable.class);
  conf.setOutputValueClass(BytesWritable.class);
  conf.setNumReduceTasks(_cluster.getNumberOfNodes());
  conf.setStrings("voldemort.index.filename",storeName + ".index");
  conf.setStrings("voldemort.data.filename",storeName + ".data");
  conf.setInt("input.data.check.percent",voldemortCheckDataPercent);
  conf.setStrings("voldemort.store.name",storeName);
  JobClient.runJob(conf);
}
