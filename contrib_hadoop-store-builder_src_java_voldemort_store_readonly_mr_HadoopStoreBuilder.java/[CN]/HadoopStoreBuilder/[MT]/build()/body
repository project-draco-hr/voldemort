{
  try {
    JobConf conf=prepareJobConf(baseJobConf);
    FileSystem fs=outputDir.getFileSystem(conf);
    if (fs.exists(outputDir)) {
      info("Deleting previous output in " + outputDir + " for building store "+ this.storeDef.getName());
      fs.delete(outputDir,true);
    }
    conf.setInt("io.file.buffer.size",DEFAULT_BUFFER_SIZE);
    conf.set("cluster.xml",new ClusterMapper().writeCluster(cluster));
    conf.set("stores.xml",new StoreDefinitionsMapper().writeStoreList(Collections.singletonList(storeDef)));
    conf.setBoolean(VoldemortBuildAndPushJob.SAVE_KEYS,saveKeys);
    conf.setBoolean(VoldemortBuildAndPushJob.REDUCER_PER_BUCKET,reducerPerBucket);
    if (!isAvro) {
      conf.setPartitionerClass(HadoopStoreBuilderPartitioner.class);
      conf.setMapperClass(mapperClass);
      conf.setMapOutputKeyClass(BytesWritable.class);
      conf.setMapOutputValueClass(BytesWritable.class);
      conf.setReducerClass(HadoopStoreBuilderReducer.class);
    }
    conf.setInputFormat(inputFormatClass);
    conf.setOutputFormat(SequenceFileOutputFormat.class);
    conf.setOutputKeyClass(BytesWritable.class);
    conf.setOutputValueClass(BytesWritable.class);
    conf.setJarByClass(getClass());
    conf.setReduceSpeculativeExecution(false);
    FileInputFormat.setInputPaths(conf,inputPath);
    conf.set("final.output.dir",outputDir.toString());
    conf.set(VoldemortBuildAndPushJob.CHECKSUM_TYPE,CheckSum.toString(checkSumType));
    conf.set("dfs.umaskmode","002");
    FileOutputFormat.setOutputPath(conf,tempDir);
    FileSystem outputFs=outputDir.getFileSystem(conf);
    if (outputFs.exists(outputDir)) {
      throw new IOException("Final output directory already exists.");
    }
    FileSystem tempFs=tempDir.getFileSystem(conf);
    tempFs.delete(tempDir,true);
    long size=sizeOfPath(tempFs,inputPath);
    logger.info("Data size = " + size + ", replication factor = "+ storeDef.getReplicationFactor()+ ", numNodes = "+ cluster.getNumberOfNodes()+ ", numPartitions = "+ cluster.getNumberOfPartitions()+ ", chunk size = "+ chunkSizeBytes);
    int numReducers;
    if (saveKeys) {
      if (this.numChunks == -1) {
        this.numChunks=Math.max((int)(storeDef.getReplicationFactor() * size / cluster.getNumberOfPartitions() / storeDef.getReplicationFactor() / chunkSizeBytes),1);
      }
 else {
        logger.info("Overriding chunk size byte and taking num chunks (" + this.numChunks + ") directly");
      }
      if (reducerPerBucket) {
        numReducers=cluster.getNumberOfPartitions() * storeDef.getReplicationFactor();
      }
 else {
        numReducers=cluster.getNumberOfPartitions() * storeDef.getReplicationFactor() * numChunks;
      }
    }
 else {
      if (this.numChunks == -1) {
        this.numChunks=Math.max((int)(storeDef.getReplicationFactor() * size / cluster.getNumberOfPartitions() / chunkSizeBytes),1);
      }
 else {
        logger.info("Overriding chunk size byte and taking num chunks (" + this.numChunks + ") directly");
      }
      if (reducerPerBucket) {
        numReducers=cluster.getNumberOfPartitions();
      }
 else {
        numReducers=cluster.getNumberOfPartitions() * numChunks;
      }
    }
    conf.setInt(VoldemortBuildAndPushJob.NUM_CHUNKS,numChunks);
    conf.setNumReduceTasks(numReducers);
    if (isAvro) {
      conf.setPartitionerClass(AvroStoreBuilderPartitioner.class);
      conf.setMapOutputKeyClass(ByteBuffer.class);
      conf.setMapOutputValueClass(ByteBuffer.class);
      conf.setInputFormat(inputFormatClass);
      conf.setOutputFormat((Class<? extends OutputFormat>)AvroOutputFormat.class);
      conf.setOutputKeyClass(ByteBuffer.class);
      conf.setOutputValueClass(ByteBuffer.class);
      AvroJob.setInputSchema(conf,Schema.parse(baseJobConf.get("avro.rec.schema")));
      AvroJob.setOutputSchema(conf,Pair.getPairSchema(Schema.create(Schema.Type.BYTES),Schema.create(Schema.Type.BYTES)));
      AvroJob.setMapperClass(conf,mapperClass);
      conf.setReducerClass(AvroStoreBuilderReducer.class);
    }
    logger.info("Number of chunks: " + numChunks + ", number of reducers: "+ numReducers+ ", save keys: "+ saveKeys+ ", reducerPerBucket: "+ reducerPerBucket);
    logger.info("Building store...");
    RunningJob job=JobClient.runJob(conf);
    Counters counters=job.getCounters();
    long numberOfRecords=counters.getCounter(Task.Counter.REDUCE_INPUT_GROUPS);
    if (numberOfRecords < minNumberOfRecords) {
      throw new VoldemortException("The number of records in the data set (" + numberOfRecords + ") is lower than the minimum required ("+ minNumberOfRecords+ "). Aborting.");
    }
    if (saveKeys) {
      logger.info("Number of collisions in the job - " + counters.getCounter(KeyValueWriter.CollisionCounter.NUM_COLLISIONS));
      logger.info("Maximum number of collisions for one entry - " + counters.getCounter(KeyValueWriter.CollisionCounter.MAX_COLLISIONS));
    }
    CheckSum checkSumGenerator=CheckSum.getInstance(this.checkSumType);
    if (!this.checkSumType.equals(CheckSumType.NONE) && checkSumGenerator == null) {
      throw new VoldemortException("Could not generate checksum digest for type " + this.checkSumType);
    }
    for (    Node node : cluster.getNodes()) {
      ReadOnlyStorageMetadata metadata=new ReadOnlyStorageMetadata();
      if (saveKeys) {
        metadata.add(ReadOnlyStorageMetadata.FORMAT,ReadOnlyStorageFormat.READONLY_V2.getCode());
      }
 else {
        metadata.add(ReadOnlyStorageMetadata.FORMAT,ReadOnlyStorageFormat.READONLY_V1.getCode());
      }
      Path nodePath=new Path(outputDir.toString(),"node-" + node.getId());
      if (!outputFs.exists(nodePath)) {
        logger.info("No data generated for node " + node.getId() + ". Generating empty folder");
        outputFs.mkdirs(nodePath);
        outputFs.setPermission(nodePath,new FsPermission(HADOOP_FILE_PERMISSION));
        logger.info("Setting permission to 755 for " + nodePath);
      }
      processCheckSumMetadataFile(node,outputFs,checkSumGenerator,nodePath,metadata);
      Path metadataPath=new Path(nodePath,".metadata");
      FSDataOutputStream metadataStream=outputFs.create(metadataPath);
      outputFs.setPermission(metadataPath,new FsPermission(HADOOP_FILE_PERMISSION));
      logger.info("Setting permission to 755 for " + metadataPath);
      metadataStream.write(metadata.toJsonString().getBytes());
      metadataStream.flush();
      metadataStream.close();
    }
  }
 catch (  Exception e) {
    logger.error("Error in Store builder",e);
    throw new VoldemortException(e);
  }
}
