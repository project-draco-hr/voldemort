{
  try {
    JobConf conf=prepareJobConf(baseJobConf);
    FileSystem fs=outputDir.getFileSystem(conf);
    if (fs.exists(outputDir)) {
      info("Deleting previous output in " + outputDir + " for building store "+ this.storeDef.getName());
      fs.delete(outputDir,true);
    }
    conf.setInt("io.file.buffer.size",DEFAULT_BUFFER_SIZE);
    conf.set("cluster.xml",new ClusterMapper().writeCluster(cluster));
    conf.set("stores.xml",new StoreDefinitionsMapper().writeStoreList(Collections.singletonList(storeDef)));
    conf.setBoolean(VoldemortBuildAndPushJob.SAVE_KEYS,saveKeys);
    conf.setBoolean(VoldemortBuildAndPushJob.REDUCER_PER_BUCKET,reducerPerBucket);
    conf.setBoolean(VoldemortBuildAndPushJob.BUILD_PRIMARY_REPLICAS_ONLY,buildPrimaryReplicasOnly);
    if (!isAvro) {
      conf.setPartitionerClass(HadoopStoreBuilderPartitioner.class);
      conf.setMapperClass(mapperClass);
      conf.setMapOutputKeyClass(BytesWritable.class);
      conf.setMapOutputValueClass(BytesWritable.class);
      conf.setReducerClass(HadoopStoreBuilderReducer.class);
    }
    conf.setInputFormat(inputFormatClass);
    conf.setOutputFormat(SequenceFileOutputFormat.class);
    conf.setOutputKeyClass(BytesWritable.class);
    conf.setOutputValueClass(BytesWritable.class);
    conf.setJarByClass(getClass());
    conf.setReduceSpeculativeExecution(false);
    FileInputFormat.setInputPaths(conf,inputPath);
    conf.set("final.output.dir",outputDir.toString());
    conf.set(VoldemortBuildAndPushJob.CHECKSUM_TYPE,CheckSum.toString(checkSumType));
    conf.set("dfs.umaskmode","002");
    FileOutputFormat.setOutputPath(conf,tempDir);
    FileSystem outputFs=outputDir.getFileSystem(conf);
    if (outputFs.exists(outputDir)) {
      throw new IOException("Final output directory already exists.");
    }
    FileSystem tempFs=tempDir.getFileSystem(conf);
    tempFs.delete(tempDir,true);
    long size=sizeOfPath(tempFs,inputPath);
    logger.info("Data size = " + size + ", replication factor = "+ storeDef.getReplicationFactor()+ ", numNodes = "+ cluster.getNumberOfNodes()+ ", numPartitions = "+ cluster.getNumberOfPartitions()+ ", chunk size = "+ chunkSizeBytes);
    int numChunks=(int)(size / cluster.getNumberOfPartitions() / chunkSizeBytes);
    int numReducers=cluster.getNumberOfPartitions();
    if (saveKeys) {
      if (buildPrimaryReplicasOnly) {
      }
 else {
        numReducers=numReducers * storeDef.getReplicationFactor();
      }
    }
 else {
      numChunks=numChunks * storeDef.getReplicationFactor();
    }
    numChunks=Math.max(numChunks,1);
    if (reducerPerBucket) {
    }
 else {
      numReducers=numReducers * numChunks;
    }
    conf.setInt(AbstractStoreBuilderConfigurable.NUM_CHUNKS,numChunks);
    conf.setNumReduceTasks(numReducers);
    if (isAvro) {
      conf.setPartitionerClass(AvroStoreBuilderPartitioner.class);
      conf.setMapOutputKeyClass(ByteBuffer.class);
      conf.setMapOutputValueClass(ByteBuffer.class);
      conf.setInputFormat(inputFormatClass);
      conf.setOutputFormat((Class<? extends OutputFormat>)AvroOutputFormat.class);
      conf.setOutputKeyClass(ByteBuffer.class);
      conf.setOutputValueClass(ByteBuffer.class);
      AvroJob.setInputSchema(conf,Schema.parse(baseJobConf.get("avro.rec.schema")));
      AvroJob.setOutputSchema(conf,Pair.getPairSchema(Schema.create(Schema.Type.BYTES),Schema.create(Schema.Type.BYTES)));
      AvroJob.setMapperClass(conf,mapperClass);
      conf.setReducerClass(AvroStoreBuilderReducer.class);
    }
    logger.info("Number of chunks: " + numChunks + ", number of reducers: "+ numReducers+ ", save keys: "+ saveKeys+ ", reducerPerBucket: "+ reducerPerBucket+ ", buildPrimaryReplicasOnly: "+ buildPrimaryReplicasOnly);
    logger.info("Building store...");
    RunningJob job=JobClient.runJob(conf);
    Counters counters=job.getCounters();
    long numberOfRecords=counters.getCounter(Task.Counter.REDUCE_INPUT_GROUPS);
    if (numberOfRecords < minNumberOfRecords) {
      throw new VoldemortException("The number of records in the data set (" + numberOfRecords + ") is lower than the minimum required ("+ minNumberOfRecords+ "). Aborting.");
    }
    if (saveKeys) {
      logger.info("Number of collisions in the job - " + counters.getCounter(KeyValueWriter.CollisionCounter.NUM_COLLISIONS));
      logger.info("Maximum number of collisions for one entry - " + counters.getCounter(KeyValueWriter.CollisionCounter.MAX_COLLISIONS));
    }
    CheckSum checkSumGenerator=CheckSum.getInstance(this.checkSumType);
    if (!this.checkSumType.equals(CheckSumType.NONE) && checkSumGenerator == null) {
      throw new VoldemortException("Could not generate checksum digest for type " + this.checkSumType);
    }
    List<String> directories=Lists.newArrayList();
    if (buildPrimaryReplicasOnly) {
      for (int partitionId=0; partitionId < cluster.getNumberOfPartitions(); partitionId++) {
        directories.add(ReadOnlyUtils.PARTITION_DIRECTORY_PREFIX + partitionId);
      }
    }
 else {
      for (      Node node : cluster.getNodes()) {
        directories.add(ReadOnlyUtils.NODE_DIRECTORY_PREFIX + node.getId());
      }
    }
    ReadOnlyStorageMetadata fullStoreMetadata=new ReadOnlyStorageMetadata();
    for (    String directoryName : directories) {
      ReadOnlyStorageMetadata metadata=new ReadOnlyStorageMetadata();
      if (saveKeys) {
        metadata.add(ReadOnlyStorageMetadata.FORMAT,ReadOnlyStorageFormat.READONLY_V2.getCode());
      }
 else {
        metadata.add(ReadOnlyStorageMetadata.FORMAT,ReadOnlyStorageFormat.READONLY_V1.getCode());
      }
      Path directoryPath=new Path(outputDir.toString(),directoryName);
      if (!outputFs.exists(directoryPath)) {
        logger.info("No data generated for " + directoryName + ". Generating empty folder");
        outputFs.mkdirs(directoryPath);
        outputFs.setPermission(directoryPath,new FsPermission(HADOOP_FILE_PERMISSION));
        logger.info("Setting permission to 755 for " + directoryPath);
      }
      processCheckSumMetadataFile(directoryName,outputFs,checkSumGenerator,directoryPath,metadata);
      if (buildPrimaryReplicasOnly) {
      }
 else {
        writeMetadataFile(directoryPath,outputFs,ReadOnlyUtils.METADATA_FILE_EXTENSION,metadata);
      }
      fullStoreMetadata.addNestedMetadata(directoryName,metadata);
    }
    writeMetadataFile(outputDir,outputFs,ReadOnlyUtils.FULL_STORE_METADATA_FILE,fullStoreMetadata);
  }
 catch (  Exception e) {
    logger.error("Error in Store builder",e);
    throw new VoldemortException(e);
  }
}
