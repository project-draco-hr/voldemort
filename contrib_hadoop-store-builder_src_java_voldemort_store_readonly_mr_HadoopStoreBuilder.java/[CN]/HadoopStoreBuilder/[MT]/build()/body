{
  try {
    JobConf conf=new JobConf(config);
    conf.setInt("io.file.buffer.size",DEFAULT_BUFFER_SIZE);
    conf.set("cluster.xml",new ClusterMapper().writeCluster(cluster));
    conf.set("stores.xml",new StoreDefinitionsMapper().writeStoreList(Collections.singletonList(storeDef)));
    conf.setBoolean("save.keys",saveKeys);
    conf.setPartitionerClass(HadoopStoreBuilderPartitioner.class);
    conf.setMapperClass(mapperClass);
    conf.setMapOutputKeyClass(BytesWritable.class);
    conf.setMapOutputValueClass(BytesWritable.class);
    conf.setReducerClass(HadoopStoreBuilderReducer.class);
    conf.setInputFormat(inputFormatClass);
    conf.setOutputFormat(SequenceFileOutputFormat.class);
    conf.setOutputKeyClass(BytesWritable.class);
    conf.setOutputValueClass(BytesWritable.class);
    conf.setJarByClass(getClass());
    conf.setReduceSpeculativeExecution(false);
    FileInputFormat.setInputPaths(conf,inputPath);
    conf.set("final.output.dir",outputDir.toString());
    conf.set("checksum.type",CheckSum.toString(checkSumType));
    FileOutputFormat.setOutputPath(conf,tempDir);
    FileSystem outputFs=outputDir.getFileSystem(conf);
    if (outputFs.exists(outputDir)) {
      throw new IOException("Final output directory already exists.");
    }
    FileSystem tempFs=tempDir.getFileSystem(conf);
    tempFs.delete(tempDir,true);
    long size=sizeOfPath(tempFs,inputPath);
    logger.info("Data size = " + size + ", replication factor = "+ storeDef.getReplicationFactor()+ ", numNodes = "+ cluster.getNumberOfNodes()+ ", chunk size = "+ chunkSizeBytes);
    int numChunks, numReducers;
    if (saveKeys) {
      numChunks=Math.max((int)(storeDef.getReplicationFactor() * size / cluster.getNumberOfPartitions() / storeDef.getReplicationFactor() / chunkSizeBytes),1);
      numReducers=cluster.getNumberOfPartitions() * storeDef.getReplicationFactor();
    }
 else {
      numChunks=Math.max((int)(storeDef.getReplicationFactor() * size / cluster.getNumberOfPartitions() / chunkSizeBytes),1);
      numReducers=cluster.getNumberOfPartitions();
    }
    conf.setInt("num.chunks",numChunks);
    conf.setNumReduceTasks(numReducers);
    if (previousDir != null) {
      if (!saveKeys) {
        logger.error("Cannot run incremental builds with this version of RO Store. Set the save-keys flag");
        throw new VoldemortException("Cannot run incremental builds with this version of RO Store. Set the save-keys flag");
      }
      for (      Node node : cluster.getNodes()) {
        Path nodePath=new Path(previousDir,"node-" + Integer.toString(node.getId()));
        FileSystem nodePathFs=nodePath.getFileSystem(conf);
        if (!nodePathFs.exists(nodePath)) {
          logger.error("No data for node " + node.getId() + " exists in "+ previousDir);
          throw new VoldemortException("No data for node " + node.getId() + " exists in "+ previousDir);
        }
        String jsonString=HadoopStoreBuilderUtils.readFileContents(nodePathFs,new Path(nodePath,".metadata"),1024);
        ReadOnlyStorageMetadata metadata=new ReadOnlyStorageMetadata(jsonString);
        String readOnlyFormatString=(String)metadata.get(ReadOnlyStorageMetadata.FORMAT);
        if (readOnlyFormatString == null || readOnlyFormatString.compareTo(ReadOnlyStorageFormat.READONLY_V2.getCode()) != 0) {
          logger.error("The read-only format for node " + node.getId() + " is not correct ");
          throw new VoldemortException("The read-only format for node " + node.getId() + " is not correct ");
        }
        int totalChunkFiles=1;
        for (        Integer partitionId : node.getPartitionIds()) {
          for (int replicaType=0; replicaType < storeDef.getReplicationFactor(); replicaType++) {
            totalChunkFiles+=HadoopStoreBuilderUtils.getChunkFiles(nodePathFs,nodePath,partitionId,replicaType).length;
          }
        }
        if (totalChunkFiles != nodePathFs.listStatus(nodePath).length) {
          logger.error("The number of chunk files is inconsistent with number expected");
          throw new VoldemortException("The number of chunk files is inconsistent with number expected");
        }
      }
      conf.set("previous.output.dir",previousDir.toString());
    }
 else {
      conf.set("previous.output.dir","");
    }
    logger.info("Number of chunks: " + numChunks + ", number of reducers: "+ numReducers+ ", save keys: "+ saveKeys);
    logger.info("Building store...");
    JobClient.runJob(conf);
    CheckSum checkSumGenerator=CheckSum.getInstance(this.checkSumType);
    if (!this.checkSumType.equals(CheckSumType.NONE) && checkSumGenerator == null) {
      throw new VoldemortException("Could not generate checksum digest for type " + this.checkSumType);
    }
    for (    Node node : cluster.getNodes()) {
      ReadOnlyStorageMetadata metadata=new ReadOnlyStorageMetadata();
      if (saveKeys) {
        metadata.add(ReadOnlyStorageMetadata.FORMAT,ReadOnlyStorageFormat.READONLY_V2.getCode());
      }
 else {
        metadata.add(ReadOnlyStorageMetadata.FORMAT,ReadOnlyStorageFormat.READONLY_V1.getCode());
      }
      Path nodePath=new Path(outputDir.toString(),"node-" + node.getId());
      if (!outputFs.exists(nodePath)) {
        logger.info("No data generated for node " + node.getId() + ". Generating empty folder");
        outputFs.mkdirs(nodePath);
      }
      if (checkSumType != CheckSumType.NONE) {
        FileStatus[] storeFiles=outputFs.listStatus(nodePath,new PathFilter(){
          public boolean accept(          Path arg0){
            if (arg0.getName().endsWith("checksum") && !arg0.getName().startsWith(".")) {
              return true;
            }
            return false;
          }
        }
);
        if (storeFiles != null && storeFiles.length > 0) {
          Arrays.sort(storeFiles,new IndexFileLastComparator());
          FSDataInputStream input=null;
          for (          FileStatus file : storeFiles) {
            try {
              input=outputFs.open(file.getPath());
              byte fileCheckSum[]=new byte[CheckSum.checkSumLength(this.checkSumType)];
              input.read(fileCheckSum);
              logger.debug("Checksum for file " + file.toString() + " - "+ new String(Hex.encodeHex(fileCheckSum)));
              checkSumGenerator.update(fileCheckSum);
            }
 catch (            Exception e) {
              logger.error("Error while reading checksum file " + e.getMessage(),e);
            }
 finally {
              if (input != null)               input.close();
            }
            outputFs.delete(file.getPath(),false);
          }
          metadata.add(ReadOnlyStorageMetadata.CHECKSUM_TYPE,CheckSum.toString(checkSumType));
          String checkSum=new String(Hex.encodeHex(checkSumGenerator.getCheckSum()));
          logger.info("Checksum for node " + node.getId() + " - "+ checkSum);
          metadata.add(ReadOnlyStorageMetadata.CHECKSUM,checkSum);
        }
      }
      FSDataOutputStream metadataStream=outputFs.create(new Path(nodePath,".metadata"));
      metadataStream.write(metadata.toJsonString().getBytes());
      metadataStream.flush();
      metadataStream.close();
    }
  }
 catch (  Exception e) {
    logger.error("Error in Store builder",e);
    throw new VoldemortException(e);
  }
}
