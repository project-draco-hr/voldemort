{
  OptionParser parser=configureParser();
  OptionSet options=parser.parse(args);
  if (options.has("help")) {
    printUsage(parser,null);
    System.exit(0);
  }
  Set<String> missing=CmdUtils.missing(options,"input","temp","mapper","cluster","storedefinitions","storename","chunksize");
  if (missing.size() > 0) {
    System.err.println("Missing required arguments: " + Joiner.on(", ").join(missing) + "\n");
    printUsage(parser,null);
    System.exit(1);
  }
  File clusterFile=new File((String)options.valueOf("cluster"));
  Cluster cluster=new ClusterMapper().readCluster(new BufferedReader(new FileReader(clusterFile)));
  File storeDefFile=new File((String)options.valueOf("storedefinitions"));
  String storeName=(String)options.valueOf("storename");
  Path inputPath=new Path((String)options.valueOf("input"));
  Path tempPath=new Path((String)options.valueOf("temp"));
  long chunkSizeBytes=Long.parseLong((String)options.valueOf("chunksize"));
  List<StoreDefinition> stores;
  stores=new StoreDefinitionsMapper().readStoreList(new BufferedReader(new FileReader(storeDefFile)));
  StoreDefinition storeDef=null;
  for (  StoreDefinition def : stores) {
    if (def.getName().equals(storeName))     storeDef=def;
  }
  if (storeDef == null) {
    System.err.println("Missing store definition for store name '" + storeName + "'");
    printUsage(parser,null);
    System.exit(1);
  }
  List<String> addJars=new ArrayList<String>();
  ClassLoader cl=Thread.currentThread().getContextClassLoader();
  if (options.has("jar")) {
    String jar=(String)options.valueOf("jar");
    URL[] urls=new URL[1];
    urls[0]=new File(jar).toURI().toURL();
    cl=new URLClassLoader(urls);
    addJars.add(jar);
  }
  Class<? extends AbstractRWHadoopStoreBuilderMapper<?,?>> mapperClass=(Class<? extends AbstractRWHadoopStoreBuilderMapper<?,?>>)ReflectUtils.loadClass((String)options.valueOf("mapper"),cl);
  Class<? extends InputFormat<?,?>> inputFormatClass=TextInputFormat.class;
  if (options.has("inputformat")) {
    String inputFormatClassName=(String)options.valueOf("inputformat");
    if (!inputFormatClassName.equalsIgnoreCase("TextInputFormat")) {
      inputFormatClass=(Class<? extends InputFormat<?,?>>)ReflectUtils.loadClass(inputFormatClassName,cl);
    }
  }
  if (inputFormatClass == null) {
    inputFormatClass=TextInputFormat.class;
  }
  int hadoopNodeId;
  if (options.has("hadoopnodeid")) {
    hadoopNodeId=Integer.parseInt((String)options.valueOf("hadoopnodeid"));
  }
 else {
    hadoopNodeId=(short)cluster.getNumberOfNodes();
  }
  int pushVersion;
  if (options.has("pushversion")) {
    pushVersion=Integer.parseInt((String)options.valueOf("pushversion"));
  }
 else {
    pushVersion=1;
  }
  Configuration conf=getConf();
  Class[] deps=new Class[]{ImmutableCollection.class,JDOMException.class,VoldemortConfig.class,HadoopRWStoreJobRunner.class,mapperClass,com.google.protobuf.Message.class,org.jdom.Content.class,com.google.common.collect.ImmutableList.class,org.apache.commons.io.IOUtils.class};
  addDepJars(conf,deps,addJars);
  HadoopRWStoreBuilder builder=new HadoopRWStoreBuilder(conf,mapperClass,inputFormatClass,cluster,storeDef,chunkSizeBytes,hadoopNodeId,pushVersion,tempPath,inputPath);
  builder.build();
  return 0;
}
