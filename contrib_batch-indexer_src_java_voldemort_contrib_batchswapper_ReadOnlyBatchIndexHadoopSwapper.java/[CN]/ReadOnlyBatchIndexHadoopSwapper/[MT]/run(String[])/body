{
  JobConf conf=new JobConf(ReadOnlyBatchIndexHadoopSwapper.class);
  configure(conf);
  Cluster cluster=null;
  try {
    cluster=ContribUtils.getVoldemortClusterDetails(conf.get("voldemort.cluster.local.filePath"));
    conf.setNumReduceTasks(0);
    conf.setMapperClass(getSwapperMapperClass());
    conf.setInputFormat(NonSplitableDummyFileInputFormat.class);
    String storeName=conf.get("voldemort.store.name");
    moveMetaDatatoHDFS(conf,new Path(conf.get("voldemort.cluster.local.filePath")));
    FileInputFormat.setInputPaths(conf,new Path(conf.get("source.path")));
    String tempHadoopDir=conf.get("hadoop.tmp.dir") + File.separatorChar + (int)(Math.random() * 1000000);
    FileOutputFormat.setOutputPath(conf,new Path(tempHadoopDir));
    JobClient.runJob(conf);
    new Path(tempHadoopDir).getFileSystem(conf).delete(new Path(tempHadoopDir),true);
    for (    Node node : cluster.getNodes()) {
      SwapperUtils.doSwap(storeName,node,conf.get("destination.path"));
    }
  }
 catch (  Exception e) {
    throw new RuntimeException("Swap Job Failed",e);
  }
  return 0;
}
