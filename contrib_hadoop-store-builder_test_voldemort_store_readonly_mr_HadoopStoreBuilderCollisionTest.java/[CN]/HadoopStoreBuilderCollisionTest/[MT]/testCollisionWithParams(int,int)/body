{
  assertEquals(totalElements % maxCollisions,0);
  Map<String,String> values=new HashMap<String,String>();
  List<String> valuesLeft=Lists.newArrayList();
  File testDir=TestUtils.createTempDir();
  File tempDir=new File(testDir,"temp");
  File outputDir=new File(testDir,"output");
  File storeDir=TestUtils.createTempDir(testDir);
  for (int i=0; i < totalElements; i++) {
    values.put(Integer.toString(i),Integer.toString(i));
    valuesLeft.add(Integer.toString(i));
  }
  String storeName="test";
  SerializerDefinition serDef=new SerializerDefinition("string");
  Cluster cluster=ServerTestUtils.getLocalCluster(1);
  Serializer<Object> serializer=(Serializer<Object>)new DefaultSerializerFactory().getSerializer(serDef);
  File inputFile=File.createTempFile("input",".txt",testDir);
  inputFile.deleteOnExit();
  StringBuilder contents=new StringBuilder();
  byte[] currentMd5=TestUtils.randomBytes(2 * ByteUtils.SIZE_OF_INT);
  int entryId=0;
  for (  Map.Entry<String,String> entry : values.entrySet()) {
    if (entryId % maxCollisions == 0) {
      currentMd5=TestUtils.randomBytes(2 * ByteUtils.SIZE_OF_INT);
    }
    contents.append(entry.getKey() + "\t" + entry.getValue()+ "\n");
    byte[] oldMd5=ByteUtils.copy(ByteUtils.md5(serializer.toBytes(entry.getKey())),0,2 * ByteUtils.SIZE_OF_INT);
    oldMd5ToNewMd5.put(new ByteArray(oldMd5),currentMd5);
    entryId++;
  }
  FileUtils.writeStringToFile(inputFile,contents.toString());
  StoreDefinition def=new StoreDefinitionBuilder().setName(storeName).setType(ReadOnlyStorageConfiguration.TYPE_NAME).setKeySerializer(serDef).setValueSerializer(serDef).setRoutingPolicy(RoutingTier.CLIENT).setRoutingStrategyType(RoutingStrategyType.CONSISTENT_STRATEGY).setReplicationFactor(1).setPreferredReads(1).setRequiredReads(1).setPreferredWrites(1).setRequiredWrites(1).build();
  HadoopStoreBuilder builder=new HadoopStoreBuilder(new Configuration(),CollidingTextStoreMapper.class,TextInputFormat.class,cluster,def,new Path(tempDir.getAbsolutePath()),new Path(outputDir.getAbsolutePath()),new Path(inputFile.getAbsolutePath()),CheckSumType.MD5,true,false,1024 * 1024 * 1024,-1,false,null);
  builder.build();
  File nodeFile=new File(outputDir,"node-0");
  File versionDir=new File(storeDir,"version-0");
  HdfsFetcher fetcher=new HdfsFetcher();
  fetcher.fetch(nodeFile.getAbsolutePath(),versionDir.getAbsolutePath());
  ReadOnlyStorageEngine engine=new ReadOnlyStorageEngine(storeName,new CustomBinarySearchStrategy(),new RoutingStrategyFactory().updateRoutingStrategy(def,cluster),0,storeDir,1);
  Store<Object,Object,Object> store=SerializingStore.wrap(engine,serializer,serializer,serializer);
  for (  Map.Entry<String,String> entry : values.entrySet()) {
    List<Versioned<Object>> found=store.get(entry.getKey(),null);
    Assert.assertEquals("Incorrect number of results",1,found.size());
    Assert.assertEquals(entry.getValue(),found.get(0).getValue());
  }
  List<String> valuesLeft2=Lists.newArrayList(valuesLeft);
  ClosableIterator<ByteArray> keyIterator=engine.keys();
  int numElements=0;
  while (keyIterator.hasNext()) {
    Object object=serializer.toObject(keyIterator.next().get());
    assertEquals(valuesLeft.remove(object),true);
    Assert.assertTrue(values.containsKey(object));
    numElements++;
  }
  Assert.assertEquals(numElements,values.size());
  Assert.assertEquals(valuesLeft.size(),0);
  ClosableIterator<Pair<ByteArray,Versioned<byte[]>>> entryIterator=engine.entries();
  numElements=0;
  while (entryIterator.hasNext()) {
    Pair<ByteArray,Versioned<byte[]>> entry=entryIterator.next();
    assertEquals(valuesLeft2.remove(serializer.toObject(entry.getFirst().get())),true);
    Assert.assertEquals(values.get(serializer.toObject(entry.getFirst().get())),serializer.toObject(entry.getSecond().getValue()));
    numElements++;
  }
  Assert.assertEquals(numElements,values.size());
  Assert.assertEquals(valuesLeft2.size(),0);
}
