{
  invokeHooks(BuildAndPushStatus.STARTING);
  if (hooks.size() > 0) {
    Thread t=new Thread(heartBeatHookRunnable);
    t.setDaemon(true);
    t.start();
  }
  try {
    boolean build=props.getBoolean(BUILD,true);
    boolean push=props.getBoolean(PUSH,true);
    checkForPreconditions(build,push);
    try {
      allClustersEqual(clusterURLs);
    }
 catch (    VoldemortException e) {
      log.error("Exception during cluster equality check",e);
      fail("Exception during cluster equality check: " + e.toString());
      return;
    }
    String reducerOutputCompressionCodec=getMatchingServerSupportedCompressionCodec(nodeId);
    if (reducerOutputCompressionCodec != null) {
      log.info("Using compression codec: " + reducerOutputCompressionCodec);
      props.put(REDUCER_OUTPUT_COMPRESS,"true");
      props.put(REDUCER_OUTPUT_COMPRESS_CODEC,reducerOutputCompressionCodec);
    }
 else {
      log.info("Using no compression");
    }
    HashMap<String,Exception> exceptions=Maps.newHashMap();
    String buildOutputDir=null;
    Map<String,Future<Boolean>> tasks=Maps.newHashMap();
    for (int index=0; index < clusterURLs.size(); index++) {
      String url=clusterURLs.get(index);
      if (isAvroJob) {
        verifyOrAddStoreAvro(url,isAvroVersioned);
      }
 else {
        verifyOrAddStore(url);
      }
      if (build) {
        if (!push || buildOutputDir == null) {
          try {
            invokeHooks(BuildAndPushStatus.BUILDING);
            buildOutputDir=runBuildStore(props,url);
          }
 catch (          Exception e) {
            log.error("Exception during build for URL: " + url,e);
            exceptions.put(url,e);
          }
        }
      }
      if (push) {
        log.info("Pushing to cluster URL: " + clusterURLs.get(index));
        if (!build) {
          buildOutputDir=dataDirs.get(index);
        }
        if (buildOutputDir == null) {
          continue;
        }
        tasks.put(url,executorService.submit(new StorePushTask(props,url,buildOutputDir)));
      }
    }
    for (    Map.Entry<String,Future<Boolean>> task : tasks.entrySet()) {
      String url=task.getKey();
      Boolean success=false;
      try {
        success=task.getValue().get();
      }
 catch (      RecoverableFailedFetchException e) {
        log.warn("There was a problem with some of the fetches, " + "but a swap was still able to go through for URL: " + url,e);
        invokeHooks(BuildAndPushStatus.SWAPPED_WITH_FAILURES,url);
      }
catch (      Exception e) {
        log.error("Exception during push for URL: " + url,e);
        exceptions.put(url,e);
      }
      if (success) {
        log.info("Successfully pushed to URL: " + url);
      }
    }
    if (build && push && buildOutputDir != null && !props.getBoolean(BUILD_OUTPUT_KEEP,false)) {
      JobConf jobConf=new JobConf();
      if (props.containsKey(HADOOP_JOB_UGI)) {
        jobConf.set(HADOOP_JOB_UGI,props.getString(HADOOP_JOB_UGI));
      }
      log.info("Informing about delete start ..." + buildOutputDir);
      HadoopUtils.deletePathIfExists(jobConf,buildOutputDir);
      log.info("Deleted " + buildOutputDir);
    }
    if (exceptions.size() == 0) {
      invokeHooks(BuildAndPushStatus.FINISHED);
      cleanUp();
    }
 else {
      log.error("Got exceptions during Build and Push:");
      for (      Map.Entry<String,Exception> entry : exceptions.entrySet()) {
        log.error("Exception for cluster: " + entry.getKey(),entry.getValue());
      }
      throw new VoldemortException("Got exceptions during Build and Push");
    }
  }
 catch (  Exception e) {
    log.error("An exception occurred during Build and Push !!",e);
    fail(e.toString());
    throw e;
  }
catch (  Throwable t) {
    log.fatal("A non-Exception Throwable was caught! (OMG) We will try to invoke hooks on a best effort basis...",t);
    fail(t.toString());
    throw new Exception("A non-Exception Throwable was caught! Bubbling it up as an Exception...",t);
  }
}
