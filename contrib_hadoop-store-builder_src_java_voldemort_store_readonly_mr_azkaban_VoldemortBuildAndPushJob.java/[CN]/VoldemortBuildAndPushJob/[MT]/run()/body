{
  invokeHooks(BuildAndPushStatus.STARTING);
  if (hooks.size() > 0) {
    Thread t=new Thread(heartBeatHookRunnable);
    t.setDaemon(true);
    t.start();
  }
  try {
    boolean build=props.getBoolean(BUILD,true);
    boolean push=props.getBoolean(PUSH,true);
    jsonKeyField=props.getString(KEY_SELECTION,null);
    jsonValueField=props.getString(VALUE_SELECTION,null);
    checkForPreconditions(build,push);
    try {
      allClustersEqual(clusterUrl);
    }
 catch (    VoldemortException e) {
      log.error("Exception during cluster equality check",e);
      fail("Exception during cluster equality check: " + e.toString());
      return;
    }
    reducerOutputCompressionCodec=getMatchingServerSupportedCompressionCodec(nodeId);
    if (reducerOutputCompressionCodec != null) {
      log.info("Using compression codec: " + reducerOutputCompressionCodec);
      props.put(REDUCER_OUTPUT_COMPRESS,"true");
      props.put(REDUCER_OUTPUT_COMPRESS_CODEC,reducerOutputCompressionCodec);
    }
 else {
      log.info("Using no compression");
    }
    HashMap<String,Exception> exceptions=Maps.newHashMap();
    String buildOutputDir=null;
    for (int index=0; index < clusterUrl.size(); index++) {
      String url=clusterUrl.get(index);
      if (isAvroJob) {
        verifyOrAddStoreAvro(url,isAvroVersioned);
      }
 else {
        verifyOrAddStore(url);
      }
      if (build) {
        if (!push || buildOutputDir == null) {
          try {
            invokeHooks(BuildAndPushStatus.BUILDING);
            buildOutputDir=runBuildStore(props,url);
          }
 catch (          Exception e) {
            log.error("Exception during build for url " + url,e);
            exceptions.put(url,e);
          }
        }
      }
      if (push) {
        if (log.isDebugEnabled()) {
          log.debug("Informing about push start ...");
        }
        log.info("Pushing to cluster url " + clusterUrl.get(index));
        try {
          if (!build) {
            buildOutputDir=dataDirs.get(index);
          }
          if (buildOutputDir == null) {
            continue;
          }
          invokeHooks(BuildAndPushStatus.PUSHING,url);
          runPushStore(props,url,buildOutputDir);
        }
 catch (        Exception e) {
          log.error("Exception during push for url " + url,e);
          exceptions.put(url,e);
        }
      }
    }
    if (build && push && buildOutputDir != null && !props.getBoolean(BUILD_OUTPUT_KEEP,false)) {
      JobConf jobConf=new JobConf();
      if (props.containsKey(HADOOP_JOB_UGI)) {
        jobConf.set(HADOOP_JOB_UGI,props.getString(HADOOP_JOB_UGI));
      }
      log.info("Informing about delete start ..." + buildOutputDir);
      HadoopUtils.deletePathIfExists(jobConf,buildOutputDir);
      log.info("Deleted " + buildOutputDir);
    }
    if (exceptions.size() == 0) {
      invokeHooks(BuildAndPushStatus.FINISHED);
      cleanUp();
    }
 else {
      log.error("Got exceptions during Build and Push:");
      for (      Map.Entry<String,Exception> entry : exceptions.entrySet()) {
        log.error("Exception for cluster: " + entry.getKey(),entry.getValue());
      }
      throw new VoldemortException("Got exceptions during Build and Push");
    }
  }
 catch (  Exception e) {
    log.error("An exception occurred during Build and Push !!",e);
    fail(e.toString());
    throw e;
  }
catch (  Throwable t) {
    log.fatal("A non-Exception Throwable was caught! (OMG) We will try to invoke hooks on a best effort basis...",t);
    fail(t.toString());
    throw new Exception("A non-Exception Throwable was caught! Bubbling it up as an Exception...",t);
  }
}
