{
  OptionParser parser=configureParser();
  OptionSet options=parser.parse(args);
  if (options.has("help")) {
    printUsage(parser,null);
    System.exit(0);
  }
  Set<String> missing=CmdUtils.missing(options,"input","output","mapper","cluster","storedefinitions","storename","chunksize");
  if (missing.size() > 0) {
    System.err.println("Missing required arguments: " + Joiner.on(", ").join(missing) + "\n");
    printUsage(parser,null);
    System.exit(1);
  }
  File clusterFile=new File((String)options.valueOf("cluster"));
  Cluster cluster=new ClusterMapper().readCluster(new BufferedReader(new FileReader(clusterFile)));
  File storeDefFile=new File((String)options.valueOf("storedefinitions"));
  String storeName=(String)options.valueOf("storename");
  List<StoreDefinition> stores;
  stores=new StoreDefinitionsMapper().readStoreList(new BufferedReader(new FileReader(storeDefFile)));
  StoreDefinition storeDef=null;
  for (  StoreDefinition def : stores) {
    if (def.getName().equals(storeName))     storeDef=def;
  }
  int replicationFactor=Integer.parseInt(CmdUtils.valueOf(options,"replication","2"));
  long chunkSizeBytes=Long.parseLong((String)options.valueOf("chunksize"));
  Path inputPath=new Path((String)options.valueOf("input"));
  Path tempDir=new Path((String)options.valueOf("tmpdir"));
  Path outputDir=new Path((String)options.valueOf("output"));
  List<String> addJars=new ArrayList<String>();
  ClassLoader cl=Thread.currentThread().getContextClassLoader();
  if (options.has("jar")) {
    String jar=(String)options.valueOf("jar");
    URL[] urls=new URL[1];
    urls[0]=new File(jar).toURI().toURL();
    cl=new URLClassLoader(urls);
    addJars.add(jar);
  }
  Class<? extends AbstractHadoopStoreBuilderMapper<?,?>> mapperClass=(Class<? extends AbstractHadoopStoreBuilderMapper<?,?>>)ReflectUtils.loadClass((String)options.valueOf("mapper"),cl);
  Class<? extends InputFormat<?,?>> inputFormatClass=TextInputFormat.class;
  if (options.has("inputformat")) {
    String inputFormatClassName=(String)options.valueOf("inputformat");
    if (!inputFormatClassName.equalsIgnoreCase("TextInputFormat")) {
      inputFormatClass=(Class<? extends InputFormat<?,?>>)ReflectUtils.loadClass(inputFormatClassName,cl);
    }
  }
  if (inputFormatClass == null) {
    inputFormatClass=TextInputFormat.class;
  }
  Configuration conf=getConf();
  if (options.has("force-overwrite")) {
    FileSystem fs=outputDir.getFileSystem(conf);
    fs.delete(outputDir,true);
  }
  CheckSumType checkSumType=CheckSumType.toType(CmdUtils.valueOf(options,"checksum",""));
  Class[] deps=new Class[]{ImmutableCollection.class,JDOMException.class,VoldemortConfig.class,HadoopStoreJobRunner.class,mapperClass};
  addDepJars(conf,deps,addJars);
  HadoopStoreBuilder builder=new HadoopStoreBuilder(conf,mapperClass,inputFormatClass,cluster,storeDef,replicationFactor,chunkSizeBytes,tempDir,outputDir,inputPath,checkSumType);
  builder.build();
  return 0;
}
